version: 0.6.2
title: |-
  A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler'
  Permissions Checker
maintainer: Peter Meissner
description: |-
  Provides functions to download and parse 'robots.txt' files.
  Ultimately the package makes it easy to check if bots
  (spiders, crawler, scrapers, ...) are allowed to access specific
  resources on a domain.
date_publication: '2018-07-18'
bug_reports: https://github.com/ropensci/robotstxt/issues
url: ''
url_cran: https://CRAN.R-project.org/package=robotstxt
url_git: https://github.com/ropensci/robotstxt
