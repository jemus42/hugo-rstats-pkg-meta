version: 0.5.1
title: Local Interpretable Model-Agnostic Explanations
maintainer: Thomas Lin Pedersen
description: |-
  When building complex models, it is often difficult to explain why
  the model should be trusted. While global measures such as accuracy are
  useful, they cannot be used for explaining why a model made a specific
  prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for
  explaining the outcome of black box models by fitting a local model around
  the point in question an perturbations of this point. The approach is
  described in more detail in the article by Ribeiro et al. (2016)
  <arXiv:1602.04938>.
date_publication: '2019-11-12'
bug_reports: https://github.com/thomasp85/lime/issues
url: https://lime.data-imaginist.com
url_cran: https://CRAN.R-project.org/package=lime
url_git: https://github.com/thomasp85/lime
