version: 0.2.1
title: Fast, Consistent Tokenization of Natural Language Text
maintainer: Lincoln Mullen
description: |-
  Convert natural language text into tokens. Includes tokenizers for
  shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs,
  characters, shingled characters, lines, tweets, Penn Treebank, regular
  expressions, as well as functions for counting characters, words, and sentences,
  and a function for splitting longer texts into separate documents, each with
  the same number of words.  The tokenizers have a consistent interface, and
  the package is built on the 'stringi' and 'Rcpp' packages for  fast
  yet correct tokenization in 'UTF-8'.
date_publication: '2018-03-29'
bug_reports: https://github.com/ropensci/tokenizers/issues
url: https://lincolnmullen.com/software/tokenizers/
url_cran: https://CRAN.R-project.org/package=tokenizers
url_git: ''
